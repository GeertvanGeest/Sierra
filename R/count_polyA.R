

###################################################################
#'
#' count polyA sites in cells
#'
#' Generates a count matrix
#'
#' @param peak.sites.file a file containing peak coordinates generated by FindPeaks
#' @param gtf.file reference (GTF) file
#' @param bamfile scRNA-seq BAM file
#' @param whitelist.file white list file
#' @param output.dir name of directory to write output (will be created if it doesn't exist)
#' @param countUMI whether to count UMIs (default: TRUE)
#' @return NULL. Writes counts to file.
#' @examples
#' CountPeaks(peak.sites.file, reference.file, bamfile, whitelist.file, output.dir)
#'
#' @importFrom magrittr "%>%"
#' @importFrom foreach "%dopar%"
#' @importFrom Matrix writeMM
#'
#' @export
CountPeaks <- function(peak.sites.file, gtf.file, bamfile, whitelist.file, output.dir, countUMI=TRUE,
			ncores = 1) {

  lock <- tempfile()
  whitelist.bc <- read.table(whitelist.file, stringsAsFactors = FALSE)
  whitelist.bc <- whitelist.bc[,1]
  n.bcs <- length(whitelist.bc)
  message("There are ", n.bcs, " whitelist barcodes.")

  n.columns <- n.bcs + 1

  # read in gene reference
  genes.ref <- make_reference(gtf.file)
  chr.names <- as.character(unique(genes.ref$chr))
  n.genes <- nrow(genes.ref)

  peak.sites <- read.table(peak.sites.file, header = T, sep = "\t",
                            stringsAsFactors = FALSE)

  # Count the peaks
  n.total.sites <- nrow(peak.sites)
  message("There are ", n.total.sites, "  sites")
  message("Doing counting for each site...")

  # Set up multiple workers
  system.name <- Sys.info()['sysname']
  new_cl <- FALSE
  if (system.name == "Windows") {
    new_cl <- TRUE
    cluster <- parallel::makePSOCKcluster(rep("localhost", ncores))
    doParallel::registerDoParallel(cluster)
  } else {
    doParallel::registerDoParallel(cores=ncores)
  }

  #print(chr.names)
  mat.to.write <- foreach::foreach(each.chr = chr.names, .combine = 'rbind') %dopar% {
      mat.per.chr <- c()
      message("Processing chr: ", each.chr)
      for(strand in c(1, -1) ) {
      message(" and strand ", strand)
      isMinusStrand <- if(strand==1) FALSE else TRUE
      peak.sites.chr <- dplyr::filter(peak.sites, Chr == each.chr & Strand == strand) %>%
                           dplyr::select(Gene, Chr, Fit.start, Fit.end, Strand)

      peak.sites.chr$Fit.start <- as.integer(peak.sites.chr$Fit.start)
      peak.sites.chr$Fit.end <- as.integer(peak.sites.chr$Fit.end)
      peak.sites.chr <- dplyr::filter(peak.sites.chr, Fit.start < Fit.end)

      # If there are no sites in this range, then just keep going
      if(nrow(peak.sites.chr) == 0) {
	      next
      }

      isMinusStrand <- if(strand==1) FALSE else TRUE
      which <- GenomicRanges::GRanges(seqnames = each.chr, ranges = IRanges::IRanges(1, max(peak.sites.chr$Fit.end) ))

      param <- Rsamtools::ScanBamParam(tag=c("CB", "UB"),
                            which = which,
                            flag=Rsamtools::scanBamFlag(isMinusStrand=isMinusStrand))

      aln <- GenomicAlignments::readGAlignments(bamfile, param=param)

      nobarcodes <- which(is.na(GenomicRanges::mcols(aln)$CB))
      noUMI <- which(is.na(GenomicRanges::mcols(aln)$UB))
      to.remove <- union(nobarcodes, noUMI)
      if (length(to.remove) > 0) {
        aln <- aln[-to.remove]
      }
      whitelist.pos <- which(GenomicRanges::mcols(aln)$CB %in% whitelist.bc)
      aln <- aln[whitelist.pos]

      # For de-duplicating UMIs, let's just remove a random read
      # when there is a duplicate
      if(countUMI) {
         GenomicRanges::mcols(aln)$CB_UB <- paste0(GenomicRanges::mcols(aln)$CB, "_", GenomicRanges::mcols(aln)$UB)
         uniqUMIs <- which(!duplicated(GenomicRanges::mcols(aln)$CB_UB))
         aln <- aln[uniqUMIs]
      }

      aln <- GenomicRanges::split(aln, GenomicRanges::mcols(aln)$CB)

      polyA.GR <- GenomicRanges::GRanges(seqnames = peak.sites.chr$Chr,
                          IRanges::IRanges(start = peak.sites.chr$Fit.start,
                                  end = as.integer(peak.sites.chr$Fit.end)))
      n.polyA <- length(polyA.GR)
      barcodes.gene <- names(aln)
      res <- sapply(barcodes.gene, function(x) GenomicRanges::countOverlaps(polyA.GR, aln[[x]]))

      # Reorder the columns of the res matrix to match the whitelist barcodes
      res.mat <- matrix(0L, nrow = n.polyA, ncol = n.bcs)
      res.mat[,match(barcodes.gene, whitelist.bc)] <- res

      # Return a sparse matrix
      mat.per.strand <- Matrix::Matrix(res.mat, sparse = TRUE)
      #mat.to.write <- matrix(0L, nrow = n.polyA, ncol = n.bcs)
      #mat.to.write[,match(barcodes.gene, whitelist.bc)] <- res
      polyA.ids <- paste0(peak.sites.chr$Gene, ":", peak.sites.chr$Chr, ":", peak.sites.chr$Fit.start,
                          "-", peak.sites.chr$Fit.end, ":", peak.sites.chr$Strand )
      rownames(mat.per.strand) <- polyA.ids


      # Need to combine the two matrices from each strand
      if(is.null(mat.per.chr)) {
         mat.per.chr <- mat.per.strand
      } else {
         mat.per.chr <- rbind(mat.per.chr, mat.per.strand)
      }
    } # Loop for strand

    # Return sparse matrix for each chromosome for combining across all threads
    return(mat.per.chr)
  } # Loop for chr
  
  if (new_cl) { ## Shut down cluster if on Windows
    ## stop cluster
    parallel::stopCluster(cluster)
  }

  if (!dir.exists(output.dir)){
    dir.create(output.dir)
  }
  writeMM(mat.to.write, file = paste0(output.dir, "/matrix.mtx"))
  write.table(whitelist.bc, file = paste0(output.dir, "/barcodes.tsv"), quote = FALSE, row.names = FALSE, col.names = FALSE)
  write.table(rownames(mat.to.write), file = paste0(output.dir, "/sitenames.tsv"), quote = FALSE, row.names = FALSE, col.names = FALSE)

} # End function



###################################################################
#'
#' Helper function
#'
#' Helper function
#'
#' @param x x
#' @return to write
#' @examples
#' make_exons(x)
make_exons <- function(x) {
  to.write <- paste0("(", as.character(x[1]), ",")
  niters <- length(x) -1
  for(i in 1:niters) {
    if(x[i+1]-x[i] == 1) {
      next
    } else {
      to.write <- paste0(to.write, as.character(x[i]), ")(",
                         as.character(x[i+1]), ",")
    }
  }
  to.write <- paste0(to.write, x[niters+1], ")")
  return(to.write)

}

###################################################################
#'
#' Build gene start-end reference from a gtf file
#'
#' Takes a GTF file as input and creates a table of chromosome start-end
#' positions for each gene. Works with GTF files downloaded from 10x Genomics website.
#'
make_reference <- function(gtf_file) {
  ## Read in the gtf file
  gtf_gr <- rtracklayer::import(gtf_file)
  gtf_TxDb <- GenomicFeatures::makeTxDbFromGFF(gtf_file, format="gtf")

  ## Build a table of gene start-end positions
  genes <- GenomicFeatures::genes(gtf_TxDb)
  genes.ref = as.data.frame(genes)

  ## Build a unique map of Ensembl ID to gene symbol
  ensembl.symbol.map = data.frame(EnsemblID = gtf_gr@elementMetadata@listData$gene_id,
                                  GeneName = gtf_gr@elementMetadata@listData$gene_name, stringsAsFactors = FALSE)
  ensembl.symbol.map %>% dplyr::distinct(EnsemblID, .keep_all = TRUE) -> ensembl.symbol.map.unique
  rownames(ensembl.symbol.map.unique) = ensembl.symbol.map.unique$EnsemblID

  ## Add gene symbol to the gene table
  ensembl.symbol.map.unique = ensembl.symbol.map.unique[rownames(genes.ref), ]
  genes.ref$Gene = ensembl.symbol.map.unique$GeneName

  ## update column names in reference file
  colnames(genes.ref) = c("chr", "start", "end", "width", "strand", "EnsemblID", "Gene")
  genes.ref = genes.ref[, c("EnsemblID", "chr", "start", "end", "strand", "Gene")]
  genes.ref$strand = plyr::mapvalues(genes.ref$strand, from = c("-", "+"), to = c("-1", "1"))

  ## Filter the chromosome names
  chr.use1 = as.character(c(1:22, c("X", "Y", "MT")))
  chr.use2 = paste0("chr", as.character(c(1:22, c("X", "Y", "MT"))))
  genes.ref = subset(genes.ref, chr %in% c(chr.use1, chr.use2) )
  genes.ref$chr <- droplevels(genes.ref$chr)

  return(genes.ref)
}

###################################################################
#'
#' Do peak calling on a scRNA-seq BAM file
#'
#' Do peak calling on a scRNA-seq BAM file...
#'
#' @param output.file a file containing polyA sites
#' @param gtf.file reference (GTF) file
#' @param bamfile scRNA-seq BAM file
#' @param junctions.file file of splice junctions (e.g. produced by regtools)
#' @param min.jcutoff minimum number of spliced reads across a junction for it to be considered (default: 50). 
#' @param min.jcutoff.prop minimum proportion of junction reads out of all junction reads for that gene (default: 0.05)
#' @param min.cov.cutoff min.cov.cutoff
#' @param min.cov.prop min.cov.prop
#' @param min.peak.cutoff min.peak.cutoff
#' @param min.peak.prop min.peak.prop
#' @param ncores number of cores to use
#' @return NULL. Writes counts to file.
#' @examples
#' FindPeaks(output.file, reference.file, bamfile, junctions.file)
#'
#' @importFrom magrittr "%>%"
#' @importFrom foreach "%dopar%"
#' @import GenomicRanges
#'
#' @export
#'
FindPeaks <- function(output.file, gtf.file, bamfile, junctions.file,
                       min.jcutoff=50, min.jcutoff.prop = 0.05, min.cov.cutoff = 500,
                       min.cov.prop = 0.05, min.peak.cutoff=200, min.peak.prop = 0.05, ncores = 1) {

  lock <- tempfile()
  #genes.ref <- read.table(reference.file,
  #                        header = TRUE, sep = ",", stringsAsFactors = FALSE)
  #chr.names <- as.character(unique(genes.ref$chr))
  #genes.ref <- subset(genes.ref, chr %in% chr.names)

  ## Read in the gtf file
  genes.ref = make_reference(gtf.file)
  n.genes = nrow(genes.ref)
  message(paste(n.genes, "gene entries to process"))

  # Initiate the output file
  write("Gene\tChr\tStrand\tMaxPosition\tFit.max.pos\tFit.start\tFit.end\tmu\tsigma\tk\texon/intron\texon.pos", file = output.file)

  # Read in the junction information
  junctions <- read.table(junctions.file, sep = "\t",header = FALSE)
  junctions <- cbind(junctions,
                     reshape2::colsplit(junctions$V11, ",", c("blocks1","blocks2")))
  junctions$start <- junctions$V2+junctions$blocks1
  junctions$end <- junctions$V3-junctions$blocks2
  junctions.GR <- GenomicRanges::GRanges(seqnames = junctions$V1,
                          IRanges::IRanges(start = junctions$start,
                                  end = junctions$end), counts = junctions$V5)
  
  # Set up multiple workers
  system.name <- Sys.info()['sysname']
  new_cl <- FALSE
  if (system.name == "Windows") {
    new_cl <- TRUE
    cluster <- parallel::makePSOCKcluster(rep("localhost", ncores))
    doParallel::registerDoParallel(cluster)
  } else {
    doParallel::registerDoParallel(cores=ncores)
  }
  
  foreach::foreach(i = 1:n.genes, .packages = c("GenomicRanges")) %dopar% {
    gene.name <- genes.ref[i, "Gene"]
    seq.name <- genes.ref[i,"chr"]
    gene.start <- genes.ref[i,"start"]
    gene.end <- genes.ref[i, "end"]
    strand <- genes.ref[i,"strand"]

    #message(i, " :", gene.name)
    isMinusStrand <- if(strand==1) FALSE else TRUE
    which <- GenomicRanges::GRanges(seqnames = seq.name, ranges = IRanges::IRanges(gene.start, gene.end))
    param <- Rsamtools::ScanBamParam(which = which,
                          flag=Rsamtools::scanBamFlag(isMinusStrand=isMinusStrand))

    aln <- GenomicAlignments::readGAlignments(bamfile, param=param)
    aln_cov <- GenomicRanges::coverage(aln)[seq.name][[1]]

    if (length(aln_cov@values) > 1) { ## check for 0 read coverage for this gene

      data <- data.frame(pos = seq(gene.start, gene.end),
                         coverage = S4Vectors::runValue(aln_cov)[S4Vectors::findRun(gene.start:gene.end, aln_cov)])

      # Find the junction which overlaps this gene
      j.cutoff <- max(min.jcutoff,min.jcutoff.prop*max(data$coverage))
      hits <- GenomicRanges::findOverlaps(which, junctions.GR)
      this.junctions.GR <- junctions.GR[hits@to]
      this.junctions.GR <- this.junctions.GR[this.junctions.GR$counts > j.cutoff]
      #this.junctions.GR <- IRanges::subset(this.junctions.GR, counts > j.cutoff)
      n.junctions <- length(this.junctions.GR)
      data.no.juncs <- data

      ## This is pretty slow way to do this filtering,
      ## can definitely improve computationally!
      if(n.junctions > 0) {
        for(i in 1:n.junctions) {
          j.start <- IRanges::start(this.junctions.GR[i])
          j.end <- IRanges::end(this.junctions.GR[i])
          data.no.juncs <- data.no.juncs %>%
            dplyr::filter(pos < j.start | pos > j.end)
        }
      }

      ## Find peaks

      totalcov <- sum(data$coverage)
      cutoff <- max(min.cov.cutoff,min.cov.prop*totalcov)
      covsum <- totalcov
      maxpeakval <- max(data$coverage)
      maxpeakcutoff <- max(min.peak.cutoff,min.peak.prop*maxpeakval )

      #message("Finding exonic sites...")
      n.points <- nrow(data.no.juncs)
      if(n.points > 0) {
        while(covsum > cutoff) {
          maxpeak <- which.max(data.no.juncs$coverage)
          if(data.no.juncs[maxpeak, "coverage"] < maxpeakcutoff) { break }
          start <- maxpeak - 300
          end <- maxpeak + 299

          if(start < 1 ) { start <- 1 }
          if(end > n.points) { end <- n.points }

          maxval <- max(data.no.juncs$coverage)
          #message(start, ",", end, ",", maxval)
          #message("length: ", data.no.juncs[end,"pos"] - data.no.juncs[start, "pos"])
          #message("max peak: ", data.no.juncs[maxpeak, "pos"])

          fit.data <- data.frame(x = seq(1,end-start+1), y = data.no.juncs[start:end,"coverage"])
          nls.res <- NULL
          tryCatch({
            nls.res <- nls( y ~ k*exp(-1/2*(x-mu)^2/sigma^2),
                            start=c(mu=300,sigma=100,k=maxval) , data = fit.data)
          }, error = function(err) { })

          if(!is.null(nls.res)) {
            residuals <- sum(summary(nls.res)$residuals )
            v <- summary(nls.res)$parameters[,"Estimate"]
            fitted.peak <- maxpeak - 300 + floor(v[1])
            from <- fitted.peak - 3*floor(v[2])
            to <- fitted.peak + 3*floor(v[2])

            # Handle the cases where the peak is too close to eithr the start or the end
            if(from < 1) { from = 1 }
            if(to > n.points) { to = n.points}

            if(fitted.peak <= 0) {
              peak.pos <- "Negative"
            } else {
              peak.pos <- data.no.juncs[fitted.peak, "pos"]
            }

            isGapped <- FALSE
            if(to <= 0) {
              to.pos <- "Negative"
            } else {
              to.pos <- data.no.juncs[to, "pos"]
              # Check if this is a spliced region
              pos.gaps <- diff(data.no.juncs[from:to, "pos"])

              if(length(which(pos.gaps > 1) > 0)) {
                isGapped <- TRUE
              }

            }


            if(isGapped) {
              exon.pos <- make_exons(data.no.juncs[from:to, "pos"])
            } else {
              exon.pos <- "NA"
            }

            line <- paste(gene.name, seq.name, strand, data.no.juncs[maxpeak, "pos"],
                          peak.pos,
                          data.no.juncs[from, "pos"],
                          to.pos,
                          v[1], v[2], v[3], "non-juncs", exon.pos, sep="\t")
            #print(line)
  	  locked <- flock::lock(lock)
            write(line,file=output.file,append=TRUE)
  	  flock::unlock(locked)
          } else {
            line <- paste(gene.name, seq.name, strand, data.no.juncs[maxpeak, "pos"],
                          "NA", "NA", "NA", "NA", "NA", "NA", "non-juncs", "NA", sep="\t")
  	  locked <- flock::lock(lock)
            write(line,file=output.file,append=TRUE)
  	  flock::unlock(locked)
          }

          data.no.juncs[start:end, "coverage"] <- 0
          covsum <- sum(data.no.juncs$coverage)
          #print(covsum)
        }
      }

      ## Now let's see if there are any peaks in the introns
      ## test each intron separate
      #message("Finding intronic peaks...")

      reduced.junctions <- GenomicRanges::reduce(this.junctions.GR)
      n.rjunctions <- length(reduced.junctions)

      if(n.junctions > 0) {

        for(i in 1:n.rjunctions) {
          #message(i)
          j.start <- IRanges::start(reduced.junctions[i])
          j.end <- IRanges::end(reduced.junctions[i])
          intron.data <- data %>%
            dplyr::filter(pos > j.start & pos < j.end)

          if(nrow(intron.data) == 0) { next }
          maxpeak <- which.max(intron.data$coverage)
          maxval <- intron.data[maxpeak, "coverage"]

          #message(maxpeak, "    ", maxval)
          if(maxval < maxpeakcutoff) { next }
          fit.data <- data.frame(x = seq(1,nrow(intron.data)),
                                 y = intron.data[,"coverage"])
          nls.res <- NULL
          tryCatch({
            nls.res <- nls( y ~ k*exp(-1/2*(x-mu)^2/sigma^2),
                            start=c(mu=maxpeak,sigma=100,k=maxval) , data = fit.data)
          }, error = function(err) { })

          if(!is.null(nls.res)) {
            residuals <- sum(summary(nls.res)$residuals )
            v <- summary(nls.res)$parameters[,"Estimate"]
            #print(v)
            fitted.peak <- floor(v[1])
            from <- fitted.peak - 3*floor(v[2])
            to <- fitted.peak + 3*floor(v[2])
            if(from < 1) { from = 1 }
            this.n.points <- nrow(intron.data)
            if(to > this.n.points) { to = this.n.points}

            if(fitted.peak <= 0) {
              peak.pos <- "Negative"
            } else {
              peak.pos <- intron.data[fitted.peak, "pos"]
            }

            if(to <= 0) {
              to.pos <- "Negative"
            } else {
              to.pos <- intron.data[to, "pos"]
            }

            line=paste(gene.name, seq.name, strand, intron.data[maxpeak, "pos"],
                       peak.pos,
                       intron.data[from, "pos"],
                       to.pos,
                       v[1], v[2], v[3], "junctions", "NA", sep="\t")

            #line=paste(gene.name, seq.name, maxpeak, v[1], v[2], v[3], "junctions", sep=",")
            #print(line)
  	  locked <- flock::lock(lock)
            write(line,file=output.file,append=TRUE)
  	  flock::unlock(locked)
          } else {
            line=paste(gene.name, seq.name, strand, intron.data[maxpeak, "pos"],
                       "NA", "NA", "NA", "NA", "NA", "NA", "junction", "NA", sep="\t")
  	  locked <- flock::lock(lock)
            write(line,file=output.file,append=TRUE)
  	  flock::unlock(locked)
          }
        }

      }

    }

  } # End loop for genes
  
  if (new_cl) { ## Shut down cluster if on Windows
    ## stop cluster
    parallel::stopCluster(cluster)
  }

  ## As a final step, read in the peak file, filter, and add Peak IDs
  peak.sites.file = output.file
  peak.sites <- read.table(peak.sites.file, header = T, sep = "\t",
                            stringsAsFactors = FALSE)

  ## Filter the polyA sites
  n.total.sites <- nrow(peak.sites)
  to.filter <- which(peak.sites$Fit.max.pos == "Negative")
  to.filter <- union(to.filter, which(peak.sites$Fit.start == "Negative"))
  to.filter <- union(to.filter, which(peak.sites$Fit.end == "Negative"))
  to.filter <- union(to.filter, which(is.na(peak.sites$Fit.start)))
  to.filter <- union(to.filter, which(is.na(peak.sites$Fit.end)))
  to.filter <- union(to.filter,  which(is.na(peak.sites$Fit.max.pos)))

  if (length(to.filter) > 0)
    peak.sites <- peak.sites[-to.filter,]
  
  ## Check for any examples of peaks with start before end
  sites.diffs <- as.numeric(peak.sites$Fit.end) - as.numeric(peak.sites$Fit.start)
  to.filter <- which(sites.diffs < 0)
  if (length(to.filter) > 0)
    peak.sites <- peak.sites[-to.filter,]
  
  n.filt.sites <- nrow(peak.sites)
  message("There are ", n.total.sites, " unfiltered sites and ", n.filt.sites, " filtered sites")

  ## Add polyA IDs to the table
  polyA.ids <- paste0(peak.sites$Gene, ":", peak.sites$Chr, ":", peak.sites$Fit.start,
                      "-", peak.sites$Fit.end, ":", peak.sites$Strand )
  peak.sites$polyA_ID = polyA.ids

  ## Remove any duplicates
  peak.sites %>% dplyr::distinct(polyA_ID, .keep_all = TRUE) -> peak.sites
  n.updated.sites = nrow(peak.sites)
  message("There are ", n.updated.sites, " sites following duplicate removal")

  ## re-write the updated table
  write.table(peak.sites, file = output.file, sep="\t", quote = FALSE, row.names = FALSE)

} # End function

###################################################################
#'
#' Aggregate multiple peak count outputs together
#'
#' Aggregate multiple peak count outputs together
#'
#' @param peak.sites.file a file containing peak site coordinates
#' @param count.dirs a list of output directories from CountPeaks
#' @param output.dir output directory for aggregate count matrix
#' @param exp.labels optional labels to append to cell barcodes corresponding to count.dirs
#' @return NULL. Writes counts to file.
#' @examples
#' AggregatePeakCounts(peak.sites.file, count.dirs, output.dir)
#'
#' @export
#'
AggregatePeakCounts <- function(peak.sites.file, count.dirs, output.dir, exp.labels = NULL) {

  if (!is.null(exp.labels)) {
    if (length(exp.labels) != length(count.dirs)) {
      stop("number of count directories should equal number of labels")
    }
  }

  peak.table <- read.table(peak.sites.file, sep="\t", header = TRUE, stringsAsFactors = FALSE)
  all.peaks <- peak.table$polyA_ID
  peaks.use <- all.peaks
  
  ## Get intersection of the peak IDs from each of the input files
  for (i in 1:length(count.dirs)) {
    this.dir <- count.dirs[i]
    sites.file <- paste0(this.dir, "/sitenames.tsv")
    this.peak.set <- readLines(sites.file)
    peaks.use <- intersect(peaks.use, this.peak.set)
    
    ## Check if peak IDs from the sites file are different to the count file
    if (length(setdiff(all.peaks, this.peak.set)) > 0 | length(setdiff(this.peak.set, all.peaks)) > 0) {
      warning(paste0("Some peaks in file ", this.dir, " do not match input peak coordinate file."))
    }
  }

  print(paste0("Aggregating counts for ", length(peaks.use), " peak coordinates"))
  aggregate.counts <- c()
  for (i in 1:length(count.dirs)) {
    this.dir <- count.dirs[i]
    this.data <- ReadPeakCounts(this.dir)

    ## update cell barcode names
    cell.names = colnames(this.data)
    barcodes = sub("(.*)-\\d", "\\1", cell.names)
    if (is.null(exp.labels)) {
      cell.names.update = paste0(barcodes, "-", i)
    } else{
      cell.names.update = paste0(barcodes, "-", exp.labels[i])
    }
    colnames(this.data) = cell.names.update

    this.data <- this.data[peaks.use, ]
    aggregate.counts <- cbind(aggregate.counts, this.data)
  }

  if (!dir.exists(output.dir)){
    dir.create(output.dir)
  }

  Matrix::writeMM(aggregate.counts, file = paste0(output.dir, "/matrix.mtx"))
  writeLines(colnames(aggregate.counts), paste0(output.dir, "/barcodes.tsv"))
  writeLines(rownames(aggregate.counts), paste0(output.dir, "/sitenames.tsv"))

}


